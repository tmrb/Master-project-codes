{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Preprocessingdata.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNpTGDCPlX111dWi7/I5Bn3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tmrb/Master-project-codes/blob/main/Preprocessingdata.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9w8HgwK0Mmt_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eaa091c3-104b-44ae-c34f-781687c0c94d"
      },
      "source": [
        "from google.colab import drive\n",
        "import sys\n",
        "drive.mount(\"/content/drive\", force_remount=True)\n",
        "root  = '/content/drive/My Drive/projeto'\n",
        "sys.path.append(root)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Er2xMJTEM5gx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77572a4b-9a00-4506-f7f4-14f0b9b63346"
      },
      "source": [
        "!pip install -U scikit-learn>=0.20\n",
        "!pip install feature_selector"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting feature_selector\n",
            "  Downloading https://files.pythonhosted.org/packages/7c/d2/5448f8af6d3507f3c455429744c6436fdf3b91d10c75f4857d8e8bb4da1c/feature_selector-1.0.0-py3-none-any.whl\n",
            "Collecting lightgbm==2.1.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bf/01/45e209af10fd16537df0c5d8a5474c286554c3eaf9ddb0ce04113f1e8506/lightgbm-2.1.1-py2.py3-none-manylinux1_x86_64.whl (711kB)\n",
            "\u001b[K     |████████████████████████████████| 716kB 21.8MB/s \n",
            "\u001b[?25hCollecting seaborn==0.8.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/10/01/dd1c7838cde3b69b247aaeb61016e238cafd8188a276e366d36aa6bcdab4/seaborn-0.8.1.tar.gz (178kB)\n",
            "\u001b[K     |████████████████████████████████| 184kB 59.0MB/s \n",
            "\u001b[?25hCollecting numpy==1.14.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3f/e7/7f24ef402a5766c677683e313c5595137d754cb9eb1c99627803280e79d5/numpy-1.14.5-cp37-cp37m-manylinux1_x86_64.whl (12.2MB)\n",
            "\u001b[K     |████████████████████████████████| 12.2MB 39.4MB/s \n",
            "\u001b[?25hCollecting matplotlib==2.1.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6d/bd/3e8cec37bcf71cfd81fe798cf733c046b1ceb123e7dddf6d3435cf03b506/matplotlib-2.1.2.tar.gz (36.2MB)\n",
            "\u001b[K     |████████████████████████████████| 36.2MB 135kB/s \n",
            "\u001b[?25hCollecting pandas==0.23.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/85/f9e4f0e47a6f1410b1d737b74a1764868e9197e3197a2be843507b505636/pandas-0.23.1.tar.gz (13.1MB)\n",
            "\u001b[K     |████████████████████████████████| 13.1MB 311kB/s \n",
            "\u001b[?25hCollecting scikit-learn==0.19.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/2c/5edf2488897cad4fb8c4ace86369833552615bf264460ae4ef6e1f258982/scikit-learn-0.19.1.tar.gz (9.5MB)\n",
            "\u001b[K     |████████████████████████████████| 9.5MB 181kB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from lightgbm==2.1.1->feature_selector) (1.4.1)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib==2.1.2->feature_selector) (1.15.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==2.1.2->feature_selector) (2.8.1)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from matplotlib==2.1.2->feature_selector) (2018.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib==2.1.2->feature_selector) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==2.1.2->feature_selector) (2.4.7)\n",
            "Building wheels for collected packages: seaborn, matplotlib, pandas, scikit-learn\n",
            "  Building wheel for seaborn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seaborn: filename=seaborn-0.8.1-cp37-none-any.whl size=184835 sha256=baf811cf23d6885b5cb62878faadd1bf84fe3212236248de957b050c694a9087\n",
            "  Stored in directory: /root/.cache/pip/wheels/26/0a/44/53ddd89769e62f7c6691976375b86c6492e7dd20a2d3970e32\n",
            "  Building wheel for matplotlib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for matplotlib: filename=matplotlib-2.1.2-cp37-cp37m-linux_x86_64.whl size=10244001 sha256=9142c9e433d01685848e899de109fc12ed2ed9b0e69f1245ba11e116bb47f28b\n",
            "  Stored in directory: /root/.cache/pip/wheels/83/9d/e6/cd76b09328833d683b447bc658dbd436bc9af0cd152c654407\n",
            "  Building wheel for pandas (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for pandas\u001b[0m\n",
            "\u001b[?25h  Running setup.py clean for pandas\n",
            "  Building wheel for scikit-learn (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for scikit-learn\u001b[0m\n",
            "\u001b[?25h  Running setup.py clean for scikit-learn\n",
            "Successfully built seaborn matplotlib\n",
            "Failed to build pandas scikit-learn\n",
            "\u001b[31mERROR: yellowbrick 0.9.1 has requirement scikit-learn>=0.20, but you'll have scikit-learn 0.19.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: xarray 0.15.1 has requirement numpy>=1.15, but you'll have numpy 1.14.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: xarray 0.15.1 has requirement pandas>=0.25, but you'll have pandas 0.23.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tifffile 2021.4.8 has requirement numpy>=1.15.1, but you'll have numpy 1.14.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.4.1 has requirement numpy~=1.19.2, but you'll have numpy 1.14.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: spacy 2.2.4 has requirement numpy>=1.15.0, but you'll have numpy 1.14.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: pyerfa 1.7.2 has requirement numpy>=1.16, but you'll have numpy 1.14.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: pyarrow 3.0.0 has requirement numpy>=1.16.6, but you'll have numpy 1.14.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: plotnine 0.6.0 has requirement matplotlib>=3.1.1, but you'll have matplotlib 2.1.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: plotnine 0.6.0 has requirement numpy>=1.16.0, but you'll have numpy 1.14.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: plotnine 0.6.0 has requirement pandas>=0.25.0, but you'll have pandas 0.23.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: numba 0.51.2 has requirement numpy>=1.15, but you'll have numpy 1.14.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: mizani 0.6.0 has requirement matplotlib>=3.1.1, but you'll have matplotlib 2.1.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: mizani 0.6.0 has requirement pandas>=0.25.0, but you'll have pandas 0.23.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: librosa 0.8.0 has requirement numpy>=1.15.0, but you'll have numpy 1.14.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: jaxlib 0.1.65+cuda110 has requirement numpy>=1.16, but you'll have numpy 1.14.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: imgaug 0.2.9 has requirement numpy>=1.15.0, but you'll have numpy 1.14.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: imbalanced-learn 0.4.3 has requirement scikit-learn>=0.20, but you'll have scikit-learn 0.19.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement pandas~=1.1.0; python_version >= \"3.0\", but you'll have pandas 0.23.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: fbprophet 0.7.1 has requirement numpy>=1.15.4, but you'll have numpy 1.14.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: fbprophet 0.7.1 has requirement pandas>=1.0.4, but you'll have pandas 0.23.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: fastai 1.0.61 has requirement numpy>=1.15, but you'll have numpy 1.14.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: cvxpy 1.0.31 has requirement numpy>=1.15, but you'll have numpy 1.14.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: blis 0.4.1 has requirement numpy>=1.15.0, but you'll have numpy 1.14.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: astropy 4.2.1 has requirement numpy>=1.17, but you'll have numpy 1.14.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: numpy, scikit-learn, lightgbm, seaborn, matplotlib, pandas, feature-selector\n",
            "  Found existing installation: numpy 1.19.5\n",
            "    Uninstalling numpy-1.19.5:\n",
            "      Successfully uninstalled numpy-1.19.5\n",
            "  Found existing installation: scikit-learn 0.24.2\n",
            "    Uninstalling scikit-learn-0.24.2:\n",
            "      Successfully uninstalled scikit-learn-0.24.2\n",
            "    Running setup.py install for scikit-learn ... \u001b[?25l\u001b[?25herror\n",
            "  Rolling back uninstall of scikit-learn\n",
            "  Moving to /usr/local/lib/python3.7/dist-packages/scikit_learn-0.24.2.dist-info/\n",
            "   from /usr/local/lib/python3.7/dist-packages/~cikit_learn-0.24.2.dist-info\n",
            "  Moving to /usr/local/lib/python3.7/dist-packages/scikit_learn.libs/\n",
            "   from /usr/local/lib/python3.7/dist-packages/~cikit_learn.libs\n",
            "  Moving to /usr/local/lib/python3.7/dist-packages/sklearn/\n",
            "   from /usr/local/lib/python3.7/dist-packages/~klearn\n",
            "\u001b[31mERROR: Command errored out with exit status 1: /usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-8rqtlytl/scikit-learn/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-8rqtlytl/scikit-learn/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /tmp/pip-record-oww3lqjw/install-record.txt --single-version-externally-managed --compile Check the logs for full command output.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRaPn27xMr0M",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "outputId": "c8c48e40-48bf-4fac-b65b-6b029b1f4162"
      },
      "source": [
        "from feature_selector import FeatureSelector\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-1c41740f15d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mfeature_selector\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFeatureSelector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'feature_selector'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9WrEZszM3tT"
      },
      "source": [
        "train = pd.read_csv('/content/drive/My Drive/projeto/overall4backup.csv',encoding='ISO-8859-1',sep = ';')\n",
        "train_labels = train['Situação']\n",
        "train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "InPiumOsNfQt"
      },
      "source": [
        "train = train.drop(columns = ['Situação'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zidLyd3gNvo6"
      },
      "source": [
        "Create the Instance\n",
        "The FeatureSelector only requires a dataset with observations in the rows and features in the columns (standard structured data). We are working with a classifified machine learning problem so we also pass in training labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mts1I8ewNnwD"
      },
      "source": [
        "fs = FeatureSelector(data = train, labels = train_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMtuQgXqN1b8"
      },
      "source": [
        "fs.identify_missing(missing_threshold=0.6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QCOAI5sOJKf"
      },
      "source": [
        "\n",
        "2. Single Unique Value\n",
        "The next method is straightforward: find any features that have only a single unique value. (This does not one-hot encode the features)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i49jNFfKOCHx"
      },
      "source": [
        "fs.identify_single_unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZQTJM_AOUU8"
      },
      "source": [
        "3. Collinear (highly correlated) Features\n",
        "This method finds pairs of collinear features based on the Pearson correlation coefficient. For each pair above the specified threshold (in terms of absolute value), it identifies one of the variables to be removed. We need to pass in a correlation_threshold.\n",
        "\n",
        "This method is based on code found at https://chrisalbon.com/machine_learning/feature_selection/drop_highly_correlated_features/\n",
        "\n",
        "For each pair, the feature that will be removed is the one that comes last in terms of the column ordering in the dataframe. (This method does not one-hot encode the data beforehand unless one_hot=True. Therefore correlations are only calculated between numeric columns)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vmvREcC9OM06"
      },
      "source": [
        "fs.identify_collinear(correlation_threshold=0.975)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vZnvsBTOYcf"
      },
      "source": [
        "correlated_features = fs.ops['collinear']\n",
        "correlated_features[:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYEQAKleOb3l"
      },
      "source": [
        "fs.plot_collinear()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGrjbfVYOhHE"
      },
      "source": [
        "fs.plot_collinear(plot_all=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwTp7rBOOsZB"
      },
      "source": [
        "fs.identify_collinear(correlation_threshold=0.98)\n",
        "fs.plot_collinear()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-n-MR-LuO37q"
      },
      "source": [
        "fs.record_collinear.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqMMnWz6PDx0"
      },
      "source": [
        "4. Zero Importance Features\n",
        "This method relies on a machine learning model to identify features to remove. It therefore requires a supervised learning problem with labels. The method works by finding feature importances using a gradient boosting machine implemented in the LightGBM library.\n",
        "\n",
        "To reduce variance in the calculated feature importances, the model is trained a default 10 times. The model is also by default trained with early stopping using a validation set (15% of the training data) to identify the optimal number of estimators to train. The following parameters can be passed to the identify_zero_importance method:\n",
        "\n",
        "task: either classification or regression. The metric and labels must match with the task\n",
        "eval_metric: the metric used for early stopping (for example auc for classification or l2 for regression). To see a list of available metrics, refer to the LightGBM docs\n",
        "n_iterations: number of training runs. The feature importances are averaged over the training runs (default = 10)\n",
        "early_stopping: whether to use early stopping when training the model (default = True). Early stopping stops training estimators (decision trees) when the performance on a validation set no longer decreases for a specified number of estimators (100 by default in this implementation). Early stopping is a form of regularization used to prevent overfitting to training data\n",
        "The data is first one-hot encoded for use in the model. This means that some of the zero importance features may be created from one-hot encoding. To view the one-hot encoded columns, we can access the one_hot_features of the FeatureSelector.\n",
        "\n",
        "Note of caution: in contrast to the other methods, the feature imporances from a model are non-deterministic (have a little randomness). The results of running this method can change each time it is run."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3O6Vv2AO_Md"
      },
      "source": [
        "fs.identify_zero_importance(task = 'classification', eval_metric = 'auc', \n",
        "                            n_iterations = 10, early_stopping = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_zaZ3CfP1zn"
      },
      "source": [
        "one_hot_features = fs.one_hot_features\n",
        "base_features = fs.base_features\n",
        "print('There are %d original features' % len(base_features))\n",
        "print('There are %d one-hot features' % len(one_hot_features))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZnWBAhjgQGrt"
      },
      "source": [
        "zero_importance_features = fs.ops['zero_importance']\n",
        "zero_importance_features[10:15]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQ-Uv7glQPmZ"
      },
      "source": [
        "**Plot Feature Importances**\n",
        "The feature importance plot using plot_feature_importances will show us the plot_n most important features (on a normalized scale where the features sum to 1). It also shows us the cumulative feature importance versus the number of features.\n",
        "\n",
        "When we plot the feature importances, we can pass in a threshold which identifies the number of features required to reach a specified cumulative feature importance. For example, threshold = 0.99 will tell us the number of features needed to account for 99% of the total importance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZUYK5eGQK39"
      },
      "source": [
        "fs.plot_feature_importances(threshold = 0.99, plot_n = 12)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lW8k-aR1QVuZ"
      },
      "source": [
        "fs.feature_importances.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFljP8AMQqOw"
      },
      "source": [
        "one_hundred_features = list(fs.feature_importances.loc[:34, 'feature'])\n",
        "len(one_hundred_features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3M3X4JE0Q-hw"
      },
      "source": [
        "**Low Importance Features**\n",
        "This method builds off the feature importances from the gradient boosting machine (identify_zero_importance must be run first) by finding the lowest importance features not needed to reach a specified cumulative total feature importance. For example, if we pass in 0.99, this will find the lowest important features that are not needed to reach 99% of the total feature importance.\n",
        "\n",
        "When using this method, we must have already run identify_zero_importance and need to pass in a cumulative_importance that accounts for that fraction of total feature importance.\n",
        "\n",
        "Note of caution: this method builds on the gradient boosting model features importances and again is non-deterministic. I advise running these two methods several times with varying parameters and testing each resulting set of features rather than picking one number and sticking to it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZNrZ-5BQ50o"
      },
      "source": [
        "fs.identify_low_importance(cumulative_importance = 0.99)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70Dmiu_pRGNA"
      },
      "source": [
        "low_importance_features = fs.ops['low_importance']\n",
        "low_importance_features[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAHISrpKRWLH"
      },
      "source": [
        "**Removing Features**\n",
        "Once we have identified the features to remove, we have a number of ways to drop the features. We can access any of the feature lists in the removal_ops dictionary and remove the columns manually. We also can use the remove method, passing in the methods that identified the features we want to remove.\n",
        "\n",
        "This method returns the resulting data which we can then use for machine learning. The original data will still be accessible in the data attribute of the Feature Selector.\n",
        "\n",
        "Be careful of the methods used for removing features! It's a good idea to inspect the features that will be removed before using the remove function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KH3ryUY2RQAn"
      },
      "source": [
        "train_no_missing = fs.remove(methods = ['missing'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2DuGja5tRzAV"
      },
      "source": [
        "train_no_missing_zero = fs.remove(methods = ['missing', 'zero_importance'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfGysq6zR2FC"
      },
      "source": [
        "all_to_remove = fs.check_removal()\n",
        "all_to_remove[10:25]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFz5q41qR8-x"
      },
      "source": [
        "train_removed = fs.remove(methods = 'all')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhFKRGtJSDII"
      },
      "source": [
        "train_removed.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NsTEmQkbSKxr"
      },
      "source": [
        "train_removed_all = fs.remove(methods = 'all', keep_one_hot=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gFxoat1SrIE"
      },
      "source": [
        "print('Original Number of Features', train.shape[1])\n",
        "print('Final Number of Features: ', train_removed_all.shape[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6s3mqfDVy4n"
      },
      "source": [
        "!fusermount -u drive\n",
        "!google-drive-ocamlfuse drive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfHvIUwHVHcO"
      },
      "source": [
        "# Import Drive API and authenticate.\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount your Drive to the Colab VM.\n",
        "drive.mount('/gdrive')\n",
        "\n",
        "# Write the DataFrame to CSV file.\n",
        "with open('/gdrive/My Drive/processedoverall.csv', 'w') as f:\n",
        "  train_removed_all.to_csv(f)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}